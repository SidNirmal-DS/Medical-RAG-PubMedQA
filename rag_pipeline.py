# -*- coding: utf-8 -*-
"""RAG_Project_PubMedQA.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MK2d6IaxfufVJMuc6-LLExo9pek0TYPl

**A Retrieval-Augmented Generation (RAG) project for medical question-answering using PubMedQA**

**Load & Explore the PubMedQA Dataset**
"""

pip install -U langchain faiss-cpu transformers torch==2.5.1 torchaudio==2.5.1 torchvision==0.20.1 sentence-transformers pymupdf datasets fastapi uvicorn fsspec==2024.6.1 gcsfs==2024.6.1

"""**Step 1: Load & Preprocess the PubMedQA Dataset**"""

from datasets import load_dataset

# Load the PubMedQA dataset from Hugging Face
dataset = load_dataset("pubmed_qa", "pqa_labeled")

# Extracting the context, questions, and answers
context_data = dataset['train']['context']
questions = dataset['train']['question']
answers = dataset['train']['final_decision']

# Displaying the first few examples
for i in range(3):
    print(f"Question: {questions[i]}")
    print(f"Context: {context_data[i]}")
    print(f"Answer: {answers[i]}")
    print("-" * 80)

pip install faiss-cpu sentence-transformers numpy

from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import pickle

# Load Sentence Transformer model
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

# Generate embeddings for the dataset
context_embeddings = embedding_model.encode(context_data, convert_to_numpy=True)

# Create a FAISS index for fast retrieval
index = faiss.IndexFlatL2(context_embeddings.shape[1])
index.add(np.array(context_embeddings))

# Save FAISS index and context data for retrieval
faiss.write_index(index, "pubmed_faiss.index")
with open("pubmed_context.pkl", "wb") as f:
    pickle.dump(context_data, f)

print("âœ… FAISS index and context data saved successfully!")

"""**Implementing RAG (Retrieval-Augmented Generation)**"""

import faiss
import pickle
from sentence_transformers import SentenceTransformer
import numpy as np

# Load FAISS index
index = faiss.read_index("pubmed_faiss.index")

# Load context data
with open("pubmed_context.pkl", "rb") as f:
    context_data = pickle.load(f)

# Load Sentence Transformer model
embedding_model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

print("âœ… FAISS index and context data loaded successfully!")

"""** Define a Function for Retrieval**"""

def retrieve_context(query, top_k=3):
    """Retrieve top-k relevant contexts from FAISS index."""
    query_embedding = embedding_model.encode([query], convert_to_numpy=True)
    distances, indices = index.search(query_embedding, top_k)

    # Extract only the 'contexts' field from the dictionary
    retrieved_texts = [context_data[i]['contexts'][0] if isinstance(context_data[i], dict) else str(context_data[i])
                       for i in indices[0]]

    return retrieved_texts

# Example test
query = "What is the role of mitochondria in programmed cell death?"
retrieved_contexts = retrieve_context(query)

print("ðŸ”¹ Retrieved Contexts:")
for i, ctx in enumerate(retrieved_contexts, 1):
    print(f"{i}. {ctx[:500]}...\n")  # Print first 500 characters

"""**Generate an Answer Using an LLM**

Now, we need to pass the retrieved contexts and the userâ€™s query to a language model (LLM) to generate a final answer.

We will use a free open-source model from Hugging Face (sentence-transformers/all-MiniLM-L6-v2) to summarize the answer.
"""

from transformers import pipeline

# Load a text-to-text generation pipeline for FLAN-T5
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-small")

def generate_answer(query, retrieved_contexts):
    """Generate an answer using the retrieved contexts."""
    context = " ".join(retrieved_contexts)  # Combine retrieved contexts
    prompt = f"Based on the following medical research:\n{context}\n\nAnswer the question: {query}"

    response = qa_pipeline(prompt, max_length=200, truncation=True)
    return response[0]['generated_text']

# Example test
query = "What is the role of mitochondria in programmed cell death?"
retrieved_contexts = retrieve_context(query)
answer = generate_answer(query, retrieved_contexts)

print("\nðŸ”¹ **Generated Answer:**")
print(answer)

from transformers import pipeline

# Load a text-to-text generation pipeline for FLAN-T5
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-small")

def generate_answer(query, retrieved_contexts):
    """Generate a detailed answer using the retrieved contexts."""
    context = "\n\n".join(retrieved_contexts)  # Join contexts with spacing
    prompt = (
        f"### Medical Research Context ###\n"
        f"{context}\n\n"
        f"### Question ###\n"
        f"{query}\n\n"
        f"### Answer ###\n"
        f"Provide a detailed medical answer based on the above research."
    )

    response = qa_pipeline(prompt, max_length=300, truncation=True)
    return response[0]['generated_text']

# Example test
query = "What is the role of mitochondria in programmed cell death?"
retrieved_contexts = retrieve_context(query)
answer = generate_answer(query, retrieved_contexts)

print("\nðŸ”¹ **Generated Answer:**")
print(answer)

from transformers import pipeline

# Load a better text-to-text model
qa_pipeline = pipeline("text2text-generation", model="google/flan-t5-large")

def generate_answer(query, retrieved_contexts):
    """Generate a well-explained answer using retrieved contexts."""
    context = "\n\n".join(retrieved_contexts)  # Join contexts with spacing

    prompt = (
        f"### Medical Research Data ###\n"
        f"{context}\n\n"
        f"### Question ###\n"
        f"{query}\n\n"
        f"### Answer ###\n"
        f"Provide a **detailed and scientific explanation** of the answer in **at least 4-5 sentences**."
    )

    response = qa_pipeline(prompt, max_length=500, truncation=True, temperature=0.7)
    return response[0]['generated_text']

# Example test
query = "What is the role of mitochondria in programmed cell death?"
retrieved_contexts = retrieve_context(query)
answer = generate_answer(query, retrieved_contexts)

print("\nðŸ”¹ **Generated Answer:**")
print(answer)

"""**Finally Got Great Response**



"""